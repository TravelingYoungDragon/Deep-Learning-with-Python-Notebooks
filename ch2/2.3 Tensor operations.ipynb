{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Python\n",
    "\n",
    "## 2.3 The gears of neural networks: tensor operations\n",
    "\n",
    "> 神经网络的“齿轮”: 张量运算\n",
    "\n",
    "在我们的第一个神经网络例子中(MNIST)，我们的每一层其实都是对输入数据做了类似如下的运算：\n",
    "\n",
    "```\n",
    "output = relu(dot(W, input) + b)\n",
    "```\n",
    "\n",
    "input 是输入，\n",
    "W 和 b 是层的属性，\n",
    "output 是输出。\n",
    "\n",
    "这些东西之间做了 relu、dot、add 运算，\n",
    "接下来我们会解释这些运算。\n",
    "\n",
    "### 逐元素操作(Element-wise)\n",
    "\n",
    "Element-wise 的操作，就是分别对张量中的每一个元素作用。\n",
    "比如，我们实现一个简单的 `relu` （`relu(x) = max(x, 0)`）:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_relu(x):\n",
    "    assert len(x.shape) == 2    # x is a 2D Numpy tensor.\n",
    "    x = x.copy()    # Avoid overwriting the input tensor.\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] = max(x[i, j], 0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加法也是逐元素操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_add(x, y):\n",
    "    # assert x and y are 2D Numpy tensors and have the same shape.\n",
    "    assert len(x.shape) == 2\n",
    "    assert x.shape == y.shape\n",
    "    \n",
    "    x = x.copy()    # Avoid overwriting the input tensor.\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[i, j]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 Numpy 里，这些都写好了。 具体的运算是交给 C 或 Fortran 写的 BLAS 进行的，速度很高。\n",
    "\n",
    "你可以这样查看有没有装 BLAS："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blas_mkl_info:\n",
      "  NOT AVAILABLE\n",
      "blis_info:\n",
      "  NOT AVAILABLE\n",
      "openblas_info:\n",
      "    libraries = ['openblas', 'openblas']\n",
      "    library_dirs = ['/usr/local/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None)]\n",
      "blas_opt_info:\n",
      "    libraries = ['openblas', 'openblas']\n",
      "    library_dirs = ['/usr/local/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None)]\n",
      "lapack_mkl_info:\n",
      "  NOT AVAILABLE\n",
      "openblas_lapack_info:\n",
      "    libraries = ['openblas', 'openblas']\n",
      "    library_dirs = ['/usr/local/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None)]\n",
      "lapack_opt_info:\n",
      "    libraries = ['openblas', 'openblas']\n",
      "    library_dirs = ['/usr/local/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.show_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是如何使用 numpy 的逐元素 relu、add："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7  9 11]\n",
      " [-3 -1 -2]\n",
      " [ 4 -1  8]]\n",
      "[[ 7  9 11]\n",
      " [ 0  0  0]\n",
      " [ 4  0  8]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1, 2, 3],\n",
    "              [-1, 2, -3],\n",
    "              [3, -1, 4]])\n",
    "b = np.array([[6, 7, 8], \n",
    "              [-2, -3, 1], \n",
    "              [1, 0, 4]])\n",
    "\n",
    "c = a + b    # Element-wise addition\n",
    "d = np.maximum(c, 0)    # Element-wise relu\n",
    "\n",
    "print(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 广播(Broadcasting)\n",
    "\n",
    "当进行逐元素运算时，如果两个张量的形状不同，在可行的情况下，较小的张量会「广播」成和较大的张量一样的形状。\n",
    "\n",
    "具体来说，可以通过广播，对形状为 `(a, b, ..., n, n+1, ..., m)` 和 `(n, n+1, ..., m)` 的两个张量进行逐元素运算。\n",
    "\n",
    "比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random((64, 3, 32, 10))    # x is a random tensor with shape (64, 3, 32, 10).\n",
    "y = np.random.random((32, 10))    # y is a random tensor with shape (32, 10).\n",
    "z = np.maximum(x, y)    # The output z has shape (64, 3, 32, 10) like x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "广播的操作如下：\n",
    "\n",
    "1. 小张量增加轴（广播轴），加到和大的一样（ndim）\n",
    "2. 小张量的元素在新轴上重复，加到和大的一样（shape）\n",
    "\n",
    "E.g. \n",
    "        \n",
    "    x: (32, 10), y: (10,)\n",
    "    Step 1: add an empty first axis to y: Y -> (1, 10)\n",
    "    Step 2: repeat y 32 times alongside this new axis: Y -> (32, 10)\n",
    "\n",
    "在完成后，有 `Y[i, :] == y for i in range(0, 32)`\n",
    "\n",
    "当然，在实际的实现里，我们不这样去复制，这样太浪费空间了，\n",
    "我们是直接在算法里实现这个“复制的”。\n",
    "比如，我们实现一个简单的向量和矩阵相加："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2,   1, 103],\n",
       "       [  5,   4, 106],\n",
       "       [  8,   7, 109]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def naive_add_matrix_and_vector(m, v):\n",
    "    assert len(m.shape) == 2    # m is a 2D Numpy tensor.\n",
    "    assert len(v.shape) == 1    # v is a Numpy vector.\n",
    "    assert m.shape[1] == v.shape[0]\n",
    "    \n",
    "    m = m.copy()\n",
    "    for i in range(m.shape[0]):\n",
    "        for j in range(m.shape[1]):\n",
    "            m[i, j] += v[j]\n",
    "    return m\n",
    "\n",
    "naive_add_matrix_and_vector(np.array([[1 ,2, 3], [4, 5, 6], [7, 8, 9]]), \n",
    "                            np.array([1, -1, 100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量点积(dot)\n",
    "\n",
    "张量点积，或者叫张量乘积，在 numpy 里用 `dot(x, y)` 完成。\n",
    "\n",
    "点积的操作可以从如下的简单程序中看出："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 向量点积\n",
    "def naive_vector_dot(x, y):\n",
    "    assert len(x.shape) == 1\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    \n",
    "    z = 0.\n",
    "    for i in range(x.shape[0]):\n",
    "        z += x[i] * y[i]\n",
    "    return z\n",
    "\n",
    "\n",
    "# 矩阵与向量点积\n",
    "def naive_matrix_vector_dot(x, y):\n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        z[i] = naive_vector_dot(x[i, :], y)\n",
    "    return z\n",
    "\n",
    "\n",
    "# 矩阵点积\n",
    "def naive_matrix_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 2\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    z = np.zeros((x.shape[0], y.shape[1]))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(y.shape[1]):\n",
    "            row_x = x[i, :]\n",
    "            column_y = y[:, j]\n",
    "            z[i, j] = naive_vector_dot(row_x, column_y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.,   1.,  22.],\n",
       "       [-13., -13., -18.],\n",
       "       [ 24.,  24.,  39.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 2, 3],\n",
    "              [-1, 2, -3],\n",
    "              [3, -1, 4]])\n",
    "b = np.array([[6, 7, 8], \n",
    "              [-2, -3, 1], \n",
    "              [1, 0, 4]])\n",
    "naive_matrix_dot(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于高维的张量点积，其实也是一样的。\n",
    "例如，(这说的是shape哈)：\n",
    "\n",
    "```\n",
    "(a, b, c, d) . (d,) -> (a, b, c)\n",
    "(a, b, c, d) . (d, e) -> (a, b, c, e)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量变形(reshaping)\n",
    "\n",
    "这个操作，简言之就是，，，还是那些元素，只是排列的方式变了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0., 1.],\n",
    "              [2., 3.],\n",
    "              [4., 5.]])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [5.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape((6, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 2.],\n",
       "       [3., 4., 5.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape((2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「转置」(transposition) 是一种特殊的矩阵变形，\n",
    "转置就是行列互换。\n",
    "\n",
    "原来的 `x[i, :]`，转置后就成了 `x[:, i]`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 300)\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros((300, 20))\n",
    "y = np.transpose(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
